trainer: mpdr
logger: base

model:
  arch: idnn_mpdr
  x_dim: 640
  z_dim: 32
  interp_dim_start: 256
  interp_dim_end: 384

  ae:
  # AE structural
    decoder:
      arch: fc
      l_hidden:
          - 128
          - 128
          - 128
          - 128
      activation: leakyrelu
      out_activation: linear
    encoder:
      arch: fc
      l_hidden:
          - 128
          - 128
          - 128
          - 128
      activation: leakyrelu
      out_activation: linear
    # Latent space
    spherical: True
    encoding_noise: 0.01
    l2_norm_reg_enc: 0.00001
    # the following two configs are not implemented for idnn yet
    # tau: 0.1
    # neg_coef: 1.

  # energy is IDNN 
  net_x:
    # should be copied from IDNN config
    arch: idnn 
    decoder:
      arch: fc
      l_hidden:
          - 128
          - 128
          - 128
          - 128
      activation: leakyrelu
      out_activation: linear
      batch_norm: False 
      out_batch_norm: False
    encoder:
      arch: fc
      l_hidden:
          - 128
          - 128
          - 128
          - 128
      activation: leakyrelu
      out_activation: linear 
      batch_norm: False 
      out_batch_norm: False 
    spherical: False 
    x_dim: 640
    z_dim: 32
    interp_dim_start: 256
    interp_dim_end: 384
  

  mpdr:
    temperature: 1
    temperature_omi: 1
    gamma_vx: null
    gamma_neg_recon: 1.
    sampling_x: langevin 
    mcmc_n_step_x: 5
    mcmc_stepsize_x: 10
    mcmc_noise_x: 0.005
      # mcmc_bound_x: [-3, 3]
    mcmc_bound_x: null  # since output of AE is linear unit..
    mcmc_custom_stepsize: True
    mcmc_n_step_omi: 40
    mcmc_stepsize_omi: 0.1
    mcmc_noise_omi: 0.01
    mcmc_normalize_omi: True 
    proj_mode: uniform 
    proj_noise_start: 0.05
    proj_noise_end: 0.3
    proj_const: 0.
    proj_const_omi: 0.0001
    proj_dist: geodesic
    l2_norm_reg_netx: null 
    use_recon_error: False
    grad_clip_off: null
    energy_method: null 

    # config specific to mpdr_conditional_idnn
    conditional: False 

    # * new configs to MPDR_Single and children *
    mcmc_n_step_x_max: null
    mcmc_n_step_x_end: null
    mcmc_n_step_s: null
    mcmc_stepsize_s: null
    mcmc_noise_s: null
    mh_omi: False 

# data
data:
    indist_train:
        dataset: DCASE 
          # path: /home3/eyj/workspace/data/dcase2020track2/dev_data/
        path: datasets/dcase2020track2/dev_data/
        split: training
        shuffle: True
        batch_size: 512 
        n_workers: 4
        # dataset specific
        machine_type: pump
        seed: 1
        is_reject_ids: False
        designate_ids: null
        normalize_dict:
            enable: True 
            scaler_pkl: null
        frames_to_concat: 5
        step: 1
        sfft_hop: 512
        reload: False  # default 경로에 있는 data, scaler을 모두 초기화 하고 싶을때
    indist_val:
        dataset: DCASE 
          # path: /home3/eyj/workspace/data/dcase2020track2/dev_data/
        path: datasets/dcase2020track2/dev_data/
        split: validation
        shuffle: False
        batch_size: 512 
        n_workers: 4
        # dataset specific
        machine_type: pump
        seed: 1
        is_reject_ids: False
        designate_ids: null
        normalize_dict:
          enable: True 
          scaler_pkl: null
        frames_to_concat: 5
        step: 1
        sfft_hop: 512
        reload: False  # default 경로에 있는 data, scaler을 모두 초기화 하고 싶을때
    ood_val:
        dataset: DCASE 
          # path: /home3/eyj/workspace/data/dcase2020track2/dev_data/
        path: datasets/dcase2020track2/dev_data/
        split: validation_ood
        shuffle: False
        batch_size: 512 
        n_workers: 4
        # dataset specific
        machine_type: pump
        seed: 1
        is_reject_ids: False
        designate_ids: null
        normalize_dict:
          enable: True 
          scaler_pkl: null
        frames_to_concat: 5
        step: 1
        sfft_hop: 512
        reload: False  # default 경로에 있는 data, scaler을 모두 초기화 하고 싶을때
    dcase_pump0:
      dataset: DCASE_test
      path: datasets/dcase2020track2/dev_data/pump
      split: evaluation  # not needed but just a placeholder
      machine_id: id_00
      frames_to_concat: 5
      step: 1
      sfft_hop: 512
      normalize_dict:
          enable: True 
          scaler_pkl: null
      batch_size: 1
      shuffle: False
      n_workers: 8
      reload: False  # default 경로에 있는 data, scaler을 모두 초기화 하고 싶을때
    dcase_pump2:
      dataset: DCASE_test
      path: datasets/dcase2020track2/dev_data/pump
      split: evaluation  # not needed but just a placeholder
      machine_id: id_02
      frames_to_concat: 5
      step: 1
      sfft_hop: 512
      normalize_dict:
          enable: True 
          scaler_pkl: null
      batch_size: 1
      shuffle: False
      n_workers: 8
      reload: False  # default 경로에 있는 data, scaler을 모두 초기화 하고 싶을때
    dcase_pump4:
      dataset: DCASE_test
      path: datasets/dcase2020track2/dev_data/pump
      split: evaluation  # not needed but just a placeholder
      machine_id: id_04
      frames_to_concat: 5
      step: 1
      sfft_hop: 512
      normalize_dict:
          enable: True 
          scaler_pkl: null
      batch_size: 1
      shuffle: False
      n_workers: 8
      reload: False  # default 경로에 있는 data, scaler을 모두 초기화 하고 싶을때
    dcase_pump6:
      dataset: DCASE_test
      path: datasets/dcase2020track2/dev_data/pump
      split: evaluation  # not needed but just a placeholder
      machine_id: id_06
      frames_to_concat: 5
      step: 1
      sfft_hop: 512
      normalize_dict:
          enable: True 
          scaler_pkl: null
      batch_size: 1
      shuffle: False
      n_workers: 8
      reload: False  # default 경로에 있는 data, scaler을 모두 초기화 하고 싶을때

training:
    load_ae: null  # you may set it null if you want to traing AE from scratch
    ae_epoch: 50
    nae_epoch: 6  # 1 epoch is about 2000 iter. we will save every epoch, so we will use either epoch 5 or epoch 6
    save_interval: 5000
    val_interval: 5000
    val_interval_nae: 1000 
    print_interval: 500
    print_interval_nae: 1000
    ae_lr: 0.001
    nae_lr: 1e-5
    mode: 'off'
    init_net_x:  results_mpdr_wo_hsemb/dcase/idnn_pump/idnn/idnn_pump_z32_nosphere_l2/run/model_epoch_100.pkl  # pre-trained idnn checkpoint

